#!/usr/bin/env python3
"""
AWS Glue Job Script for Iceberg Migration
This script reads the configuration file generated by the Streamlit app
and creates an Iceberg table from the source Glue table.
"""

import sys
import json
import boto3
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from pyspark.sql import DataFrame
from pyspark.sql.functions import *
from pyspark.sql.types import *

# Initialize Glue context
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)

def load_config_from_s3(s3_path: str) -> dict:
    """Load configuration from S3"""
    s3_client = boto3.client('s3')
    
    # Parse S3 path
    if s3_path.startswith('s3://'):
        s3_path = s3_path[5:]  # Remove 's3://' prefix
    
    bucket, key = s3_path.split('/', 1)
    
    # Download and parse config
    response = s3_client.get_object(Bucket=bucket, Key=key)
    config_content = response['Body'].read().decode('utf-8')
    return json.loads(config_content)

def create_iceberg_table(spark, config: dict):
    """Create Iceberg table based on configuration"""
    
    # Check if this is migration or direct creation
    mode = config.get('mode', 'migration')
    
    if mode == 'direct_creation':
        return create_direct_iceberg_table(spark, config)
    else:
        return create_migration_iceberg_table(spark, config)

def create_direct_iceberg_table(spark, config: dict):
    """Create Iceberg table from scratch based on schema definition"""
    
    # Extract configuration
    schema = config['schema']
    partitioning = config['partitioning']
    target_config = config['target']
    iceberg_settings = config['iceberg_settings']
    
    # Set Iceberg catalog properties
    catalog_name = iceberg_settings.get('catalog_name', 'glue_catalog')
    warehouse_location = iceberg_settings.get('warehouse_location')
    
    # Configure Spark for Iceberg
    spark.conf.set(f"spark.sql.catalog.{catalog_name}", "org.apache.iceberg.spark.SparkCatalog")
    spark.conf.set(f"spark.sql.catalog.{catalog_name}.warehouse", warehouse_location)
    spark.conf.set(f"spark.sql.catalog.{catalog_name}.catalog-impl", "org.apache.iceberg.aws.glue.GlueCatalog")
    spark.conf.set(f"spark.sql.catalog.{catalog_name}.io-impl", "org.apache.iceberg.aws.s3.S3FileIO")
    
    # Get target configuration
    target_database = target_config['target_database']
    target_table = target_config['target_table']
    target_s3_location = target_config['target_s3_location']
    
    # Create target database if it doesn't exist
    print(f"Creating target database: {target_database}")
    spark.sql(f"CREATE DATABASE IF NOT EXISTS {catalog_name}.{target_database}")
    
    # Configure Iceberg table properties
    table_properties = {
        'write.format.default': iceberg_settings.get('file_format', 'parquet'),
        'write.target-file-size-bytes': str(iceberg_settings.get('target_file_size', 128) * 1024 * 1024),
        'write.distribution-mode': 'hash',
        'write.parquet.compression-codec': iceberg_settings.get('compression', 'snappy'),
        'history.expire.max-snapshot-age-ms': str(iceberg_settings.get('history_retention_days', 30) * 24 * 60 * 60 * 1000),
        'snapshot.time-retention.millis': str(iceberg_settings.get('snapshot_retention_days', 7) * 24 * 60 * 60 * 1000)
    }
    
    # Add compaction properties if enabled
    if iceberg_settings.get('compaction_enabled', True):
        table_properties.update({
            'write.merge.mode': 'copy-on-write',
            'write.merge.distribution-mode': 'hash',
            'write.target-file-size-bytes': str(iceberg_settings.get('compaction_target_size', 512) * 1024 * 1024),
            'write.merge.mode': 'copy-on-write'
        })
    
    # Create table properties string
    properties_string = ', '.join([f"'{k}' = '{v}'" for k, v in table_properties.items()])
    
    # Create column definitions
    columns = []
    for col in schema:
        col_name = col['name']
        col_type = col['type']
        nullable = "NULL" if col.get('nullable', True) else "NOT NULL"
        comment = f" COMMENT '{col.get('comment', '')}'" if col.get('comment') else ""
        columns.append(f"{col_name} {col_type} {nullable}{comment}")
    
    columns_string = ', '.join(columns)
    
    # Add partitioning if configured
    partition_clause = ""
    if partitioning.get('strategy') != 'none' and partitioning.get('columns'):
        partition_columns = partitioning['columns']
        if partitioning['strategy'] == 'bucket':
            bucket_count = iceberg_settings.get('bucket_count', 10)
            partition_clause = f" PARTITIONED BY BUCKET {bucket_count} ({', '.join(partition_columns)})"
        elif partitioning['strategy'] == 'truncate':
            truncate_width = iceberg_settings.get('truncate_width', 10)
            partition_clause = f" PARTITIONED BY TRUNCATE {truncate_width} ({', '.join(partition_columns)})"
        elif partitioning['strategy'] in ['year', 'month', 'day', 'hour']:
            partition_clause = f" PARTITIONED BY {partitioning['strategy'].upper()} ({', '.join(partition_columns)})"
        else:  # identity
            partition_clause = f" PARTITIONED BY ({', '.join(partition_columns)})"
    
    # Create table SQL
    create_table_sql = f"""
    CREATE TABLE {catalog_name}.{target_database}.{target_table} (
        {columns_string}
    ) USING iceberg
    LOCATION '{target_s3_location}'
    TBLPROPERTIES ({properties_string}){partition_clause}
    """
    
    print("Executing table creation SQL:")
    print(create_table_sql)
    
    spark.sql(create_table_sql)
    
    print(f"Successfully created Iceberg table: {target_database}.{target_table}")
    
    return True

def create_migration_iceberg_table(spark, config: dict):
    """Create Iceberg table by migrating from existing Glue table"""
    
    # Extract configuration
    source_config = config['source']
    target_config = config['target']
    iceberg_settings = config['iceberg_settings']
    data_type_mapping = config['data_type_mapping']
    
    # Set Iceberg catalog properties
    catalog_name = iceberg_settings.get('catalog_name', 'glue_catalog')
    warehouse_location = iceberg_settings.get('warehouse_location')
    
    # Configure Spark for Iceberg
    spark.conf.set(f"spark.sql.catalog.{catalog_name}", "org.apache.iceberg.spark.SparkCatalog")
    spark.conf.set(f"spark.sql.catalog.{catalog_name}.warehouse", warehouse_location)
    spark.conf.set(f"spark.sql.catalog.{catalog_name}.catalog-impl", "org.apache.iceberg.aws.glue.GlueCatalog")
    spark.conf.set(f"spark.sql.catalog.{catalog_name}.io-impl", "org.apache.iceberg.aws.s3.S3FileIO")
    
    # Read source table
    source_database = source_config['database']
    source_table = source_config['table']
    
    print(f"Reading source table: {source_database}.{source_table}")
    source_df = spark.table(f"{source_database}.{source_table}")
    
    # Apply data type mappings
    print("Applying data type mappings...")
    for column_name, mapping in data_type_mapping.items():
        glue_type = mapping['glue_type']
        iceberg_type = mapping['iceberg_type']
        
        if glue_type.lower() != iceberg_type.lower():
            print(f"Mapping {column_name}: {glue_type} -> {iceberg_type}")
            # Apply type casting if needed
            if iceberg_type == 'string':
                source_df = source_df.withColumn(column_name, col(column_name).cast(StringType()))
            elif iceberg_type == 'int':
                source_df = source_df.withColumn(column_name, col(column_name).cast(IntegerType()))
            elif iceberg_type == 'long':
                source_df = source_df.withColumn(column_name, col(column_name).cast(LongType()))
            elif iceberg_type == 'float':
                source_df = source_df.withColumn(column_name, col(column_name).cast(FloatType()))
            elif iceberg_type == 'double':
                source_df = source_df.withColumn(column_name, col(column_name).cast(DoubleType()))
            elif iceberg_type == 'boolean':
                source_df = source_df.withColumn(column_name, col(column_name).cast(BooleanType()))
            elif iceberg_type == 'date':
                source_df = source_df.withColumn(column_name, col(column_name).cast(DateType()))
            elif iceberg_type == 'timestamp':
                source_df = source_df.withColumn(column_name, col(column_name).cast(TimestampType()))
    
    # Get target configuration
    target_database = target_config['target_database']
    target_table = target_config['target_table']
    target_s3_location = target_config['target_s3_location']
    
    # Create target database if it doesn't exist
    print(f"Creating target database: {target_database}")
    spark.sql(f"CREATE DATABASE IF NOT EXISTS {catalog_name}.{target_database}")
    
    # Configure Iceberg table properties
    table_properties = {
        'write.format.default': iceberg_settings.get('file_format', 'parquet'),
        'write.target-file-size-bytes': str(iceberg_settings.get('target_file_size', 128) * 1024 * 1024),
        'write.distribution-mode': 'hash',
        'write.parquet.compression-codec': iceberg_settings.get('compression', 'snappy'),
        'history.expire.max-snapshot-age-ms': str(iceberg_settings.get('history_retention_days', 30) * 24 * 60 * 60 * 1000),
        'snapshot.time-retention.millis': str(iceberg_settings.get('snapshot_retention_days', 7) * 24 * 60 * 60 * 1000)
    }
    
    # Enable compaction if configured
    if iceberg_settings.get('compaction_enabled', True):
        table_properties['write.merge.mode'] = 'copy-on-write'
        table_properties['write.merge.distribution-mode'] = 'hash'
    
    # Create table properties string
    properties_string = ', '.join([f"'{k}' = '{v}'" for k, v in table_properties.items()])
    
    # Create Iceberg table
    print(f"Creating Iceberg table: {target_database}.{target_table}")
    
    # Get column definitions
    columns = []
    for column_name, mapping in data_type_mapping.items():
        iceberg_type = mapping['iceberg_type']
        columns.append(f"{column_name} {iceberg_type}")
    
    columns_string = ', '.join(columns)
    
    # Create table SQL
    create_table_sql = f"""
    CREATE TABLE {catalog_name}.{target_database}.{target_table} (
        {columns_string}
    ) USING iceberg
    LOCATION '{target_s3_location}'
    TBLPROPERTIES ({properties_string})
    """
    
    print("Executing table creation SQL:")
    print(create_table_sql)
    
    spark.sql(create_table_sql)
    
    # Write data to Iceberg table
    print("Writing data to Iceberg table...")
    source_df.writeTo(f"{catalog_name}.{target_database}.{target_table}").append()
    
    print(f"Successfully created Iceberg table: {target_database}.{target_table}")
    
    # Verify table creation
    result = spark.sql(f"SELECT COUNT(*) as row_count FROM {catalog_name}.{target_database}.{target_table}")
    row_count = result.collect()[0]['row_count']
    print(f"Table contains {row_count} rows")
    
    return True

def main():
    """Main function"""
    try:
        # Get job parameters
        args = getResolvedOptions(sys.argv, ['config-s3-path'])
        config_s3_path = args['config-s3-path']
        
        print(f"Loading configuration from: {config_s3_path}")
        config = load_config_from_s3(config_s3_path)
        
        print("Configuration loaded successfully")
        print(f"Source: {config['source']['database']}.{config['source']['table']}")
        print(f"Target: {config['target']['target_database']}.{config['target']['target_table']}")
        
        # Create Iceberg table
        success = create_iceberg_table(spark, config)
        
        if success:
            print("Migration completed successfully!")
        else:
            print("Migration failed!")
            sys.exit(1)
            
    except Exception as e:
        print(f"Error during migration: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    
    finally:
        job.commit()

if __name__ == "__main__":
    main()

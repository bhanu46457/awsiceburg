#!/usr/bin/env python3
"""
AWS Glue Job Script for Iceberg Migration
This script reads the configuration file generated by the Streamlit app
and creates an Iceberg table from the source Glue table with optimization features.
"""

import sys
import json
import boto3
import time
from datetime import datetime, timedelta
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from pyspark.sql import DataFrame
from pyspark.sql.functions import *
from pyspark.sql.types import *

# Initialize Glue context
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)

# Initialize AWS clients
glue_client = boto3.client('glue')
s3_client = boto3.client('s3')
sts_client = boto3.client('sts')

def load_config_from_s3(s3_path: str) -> dict:
    """Load configuration from S3"""
    s3_client = boto3.client('s3')
    
    # Parse S3 path
    if s3_path.startswith('s3://'):
        s3_path = s3_path[5:]  # Remove 's3://' prefix
    
    bucket, key = s3_path.split('/', 1)
    
    # Download and parse config
    response = s3_client.get_object(Bucket=bucket, Key=key)
    config_content = response['Body'].read().decode('utf-8')
    return json.loads(config_content)

def validate_optimizer_configuration(iceberg_settings: dict) -> bool:
    """
    Validate the optimizer configuration parameters
    """
    print("Validating optimizer configuration...")
    
    # Validate run rate hours (must be between 3 and 168)
    run_rate_hours = iceberg_settings.get('retention_run_rate_hours', 24)
    if not (3 <= run_rate_hours <= 168):
        print(f"❌ Invalid retention_run_rate_hours: {run_rate_hours}. Must be between 3 and 168 hours.")
        return False
    
    orphan_run_rate_hours = iceberg_settings.get('orphan_file_run_rate_hours', 24)
    if not (3 <= orphan_run_rate_hours <= 168):
        print(f"❌ Invalid orphan_file_run_rate_hours: {orphan_run_rate_hours}. Must be between 3 and 168 hours.")
        return False
    
    # Validate compaction strategy
    valid_strategies = ['binpack', 'sort', 'z-order']
    strategy = iceberg_settings.get('compaction_strategy', 'binpack')
    if strategy not in valid_strategies:
        print(f"❌ Invalid compaction_strategy: {strategy}. Must be one of: {valid_strategies}")
        return False
    
    print("✅ Configuration validation passed")
    return True

def enable_table_optimizations(database_name: str, table_name: str, iceberg_settings: dict):
    """
    Enable table optimizations using the create_table_optimizer API for the latest Glue implementation
    """
    print(f"Enabling table optimizations for {database_name}.{table_name}")
    
    # Validate configuration first
    if not validate_optimizer_configuration(iceberg_settings):
        print("❌ Configuration validation failed. Skipping optimizer creation.")
        return False
    
    try:
        # Get the catalog ID (account ID)
        catalog_id = sts_client.get_caller_identity()['Account']
        
        # Get the IAM role ARN from the Glue job context
        role_arn = f"arn:aws:iam::{catalog_id}:role/GlueServiceRole"
        
        # Try to get the role from the job context
        try:
            job_name = job.getJobName() if hasattr(job, 'getJobName') else None
            if job_name:
                job_response = glue_client.get_job(JobName=job_name)
                role_arn = job_response['Job']['Role']
        except:
            print(f"Using default role ARN: {role_arn}")
        
        optimizers_created = []
        
        # Enable Compaction Optimization
        if iceberg_settings.get('compaction_enabled', True):
            print("Creating compaction optimizer...")
            # Build compaction configuration
            compaction_config = {
                'icebergConfiguration': {
                    'strategy': iceberg_settings.get('compaction_strategy', 'binpack'),
                    'minInputFiles': iceberg_settings.get('compaction_min_file_count', 100),
                    'deleteFileThreshold': iceberg_settings.get('compaction_delete_file_threshold', 1)
                }
            }
            
            try:
                # Build the complete configuration
                optimizer_config = {
                    'enabled': True,
                    'roleArn': role_arn,
                    'compactionConfiguration': compaction_config
                }
                
                # Add VPC configuration if specified
                if iceberg_settings.get('vpc_connection_name'):
                    optimizer_config['vpcConfiguration'] = {
                        'glueConnectionName': iceberg_settings.get('vpc_connection_name')
                    }
                
                response = glue_client.create_table_optimizer(
                    CatalogId=catalog_id,
                    DatabaseName=database_name,
                    TableName=table_name,
                    Type='compaction',
                    TableOptimizerConfiguration=optimizer_config
                )
                optimizers_created.append('COMPACTION')
                print("✅ Compaction optimizer created successfully")
            except Exception as e:
                print(f"❌ Failed to create compaction optimizer: {str(e)}")
        
        # Enable Snapshot Retention Optimization
        print("Creating snapshot retention optimizer...")
        # Build retention configuration
        retention_config = {
            'icebergConfiguration': {
                'snapshotRetentionPeriodInDays': iceberg_settings.get('snapshot_retention_days', 5),
                'numberOfSnapshotsToRetain': iceberg_settings.get('number_of_snapshots_to_retain', 1),
                'cleanExpiredFiles': iceberg_settings.get('clean_expired_files', True),
                'runRateInHours': iceberg_settings.get('retention_run_rate_hours', 24)
            }
        }
        
        try:
            # Build the complete configuration
            optimizer_config = {
                'enabled': True,
                'roleArn': role_arn,
                'retentionConfiguration': retention_config
            }
            
            # Add VPC configuration if specified
            if iceberg_settings.get('vpc_connection_name'):
                optimizer_config['vpcConfiguration'] = {
                    'glueConnectionName': iceberg_settings.get('vpc_connection_name')
                }
            
            response = glue_client.create_table_optimizer(
                CatalogId=catalog_id,
                DatabaseName=database_name,
                TableName=table_name,
                Type='retention',
                TableOptimizerConfiguration=optimizer_config
            )
            optimizers_created.append('RETENTION')
            print("✅ Snapshot retention optimizer created successfully")
        except Exception as e:
            print(f"❌ Failed to create snapshot retention optimizer: {str(e)}")
        
        # Enable Orphan File Deletion Optimization
        print("Creating orphan file deletion optimizer...")
        # Build orphan file deletion configuration
        orphan_file_config = {
            'orphanFileRetentionPeriodInDays': iceberg_settings.get('orphan_file_retention_days', 3),
            'runRateInHours': iceberg_settings.get('orphan_file_run_rate_hours', 24)
        }
        
        # Only add location if specified (optional parameter)
        orphan_file_location = iceberg_settings.get('orphan_file_location', '')
        if orphan_file_location:
            orphan_file_config['location'] = orphan_file_location
        
        orphan_file_deletion_config = {
            'icebergConfiguration': orphan_file_config
        }
        
        try:
            # Build the complete configuration
            optimizer_config = {
                'enabled': True,
                'roleArn': role_arn,
                'orphanFileDeletionConfiguration': orphan_file_deletion_config
            }
            
            # Add VPC configuration if specified
            if iceberg_settings.get('vpc_connection_name'):
                optimizer_config['vpcConfiguration'] = {
                    'glueConnectionName': iceberg_settings.get('vpc_connection_name')
                }
            
            response = glue_client.create_table_optimizer(
                CatalogId=catalog_id,
                DatabaseName=database_name,
                TableName=table_name,
                Type='orphan_file_deletion',
                TableOptimizerConfiguration=optimizer_config
            )
            optimizers_created.append('ORPHAN_FILE_DELETION')
            print("✅ Orphan file deletion optimizer created successfully")
        except Exception as e:
            print(f"❌ Failed to create orphan file deletion optimizer: {str(e)}")
        
        print(f"Successfully created {len(optimizers_created)} optimizers: {', '.join(optimizers_created)}")
        
        # Log configuration summary
        print("\n" + "="*50)
        print("OPTIMIZER CONFIGURATION SUMMARY")
        print("="*50)
        print(f"Compaction Strategy: {iceberg_settings.get('compaction_strategy', 'binpack')}")
        print(f"Min Input Files: {iceberg_settings.get('compaction_min_file_count', 100)}")
        print(f"Delete File Threshold: {iceberg_settings.get('compaction_delete_file_threshold', 1)}")
        print(f"Snapshot Retention Days: {iceberg_settings.get('snapshot_retention_days', 5)}")
        print(f"Number of Snapshots to Retain: {iceberg_settings.get('number_of_snapshots_to_retain', 1)}")
        print(f"Clean Expired Files: {iceberg_settings.get('clean_expired_files', True)}")
        print(f"Retention Run Rate (hours): {iceberg_settings.get('retention_run_rate_hours', 24)}")
        print(f"Orphan File Retention Days: {iceberg_settings.get('orphan_file_retention_days', 3)}")
        print(f"Orphan File Run Rate (hours): {iceberg_settings.get('orphan_file_run_rate_hours', 24)}")
        if iceberg_settings.get('vpc_connection_name'):
            print(f"VPC Connection: {iceberg_settings.get('vpc_connection_name')}")
        print("="*50)
        
        return len(optimizers_created) > 0
        
    except Exception as e:
        print(f"Error enabling table optimizations: {str(e)}")
        return False

def verify_optimization_status(database_name: str, table_name: str):
    """
    Verify that table optimizers are properly created and configured
    """
    print(f"Verifying optimization status for {database_name}.{table_name}")
    
    try:
        # Get the catalog ID
        catalog_id = sts_client.get_caller_identity()['Account']
        
        # Check each optimizer type individually since list_table_optimizers doesn't exist
        optimizer_types = ['compaction', 'retention', 'orphan_file_deletion']
        optimizer_status = {}
        
        print("Table Optimizers Status:")
        
        for optimizer_type in optimizer_types:
            try:
                response = glue_client.get_table_optimizer(
                    CatalogId=catalog_id,
                    DatabaseName=database_name,
                    TableName=table_name,
                    Type=optimizer_type
                )
                
                optimizer = response.get('TableOptimizer', {})
                status = optimizer.get('Status', 'UNKNOWN')
                enabled = optimizer.get('Enabled', False)
                
                status_icon = "✅" if enabled and status == 'ACTIVE' else "❌"
                print(f"  {status_icon} {optimizer_type}: {status} (Enabled: {enabled})")
                
                optimizer_status[optimizer_type.lower()] = {
                    'enabled': enabled,
                    'status': status
                }
                
            except Exception as e:
                print(f"  ❌ {optimizer_type}: Not found or error - {str(e)}")
                optimizer_status[optimizer_type.lower()] = {
                    'enabled': False,
                    'status': 'NOT_FOUND'
                }
        
        return optimizer_status
        
    except Exception as e:
        print(f"Error verifying optimization status: {str(e)}")
        return {}

def monitor_optimization_jobs(database_name: str, table_name: str):
    """
    Monitor table optimizer runs and their status
    """
    print(f"Monitoring optimization runs for {database_name}.{table_name}")
    
    try:
        # Get the catalog ID
        catalog_id = sts_client.get_caller_identity()['Account']
        
        # Check each optimizer type individually
        optimizer_types = ['compaction', 'retention', 'orphan_file_deletion']
        found_optimizers = []
        
        print(f"Checking {len(optimizer_types)} optimizer types:")
        
        for optimizer_type in optimizer_types:
            try:
                response = glue_client.get_table_optimizer(
                    CatalogId=catalog_id,
                    DatabaseName=database_name,
                    TableName=table_name,
                    Type=optimizer_type
                )
                
                optimizer = response.get('TableOptimizer', {})
                status = optimizer.get('Status', 'UNKNOWN')
                enabled = optimizer.get('Enabled', False)
                
                print(f"  📊 {optimizer_type}:")
                print(f"    Status: {status}")
                print(f"    Enabled: {enabled}")
                
                # Note: list_table_optimizer_runs doesn't exist in the current API
                # We can only get the optimizer status, not individual runs
                print(f"    Note: Individual run history not available in current API")
                
                found_optimizers.append(optimizer)
                    
            except Exception as e:
                print(f"  ❌ {optimizer_type}: Not found or error - {str(e)}")
        
        if not found_optimizers:
            print("No table optimizers found for this table")
        
        return found_optimizers
        
    except Exception as e:
        print(f"Error monitoring optimization jobs: {str(e)}")
        return []

def check_existing_optimizers(database_name: str, table_name: str):
    """
    Check if table optimizers already exist for this table
    """
    try:
        catalog_id = sts_client.get_caller_identity()['Account']
        
        # Check each optimizer type individually since list_table_optimizers doesn't exist
        optimizer_types = ['compaction', 'retention', 'orphan_file_deletion']
        existing_optimizers = []
        
        for optimizer_type in optimizer_types:
            try:
                response = glue_client.get_table_optimizer(
                    CatalogId=catalog_id,
                    DatabaseName=database_name,
                    TableName=table_name,
                    Type=optimizer_type
                )
                
                optimizer = response.get('TableOptimizer', {})
                if optimizer:
                    existing_optimizers.append(optimizer)
                    print(f"  - {optimizer_type}: {optimizer.get('Status', 'UNKNOWN')}")
                    
            except Exception as e:
                # Optimizer doesn't exist, which is fine
                pass
        
        if existing_optimizers:
            print(f"Found {len(existing_optimizers)} existing optimizers:")
            return existing_optimizers
        else:
            print("No existing optimizers found")
            return []
            
    except Exception as e:
        print(f"Error checking existing optimizers: {str(e)}")
        return []

def create_iceberg_table(spark, config: dict):
    """Create Iceberg table based on configuration"""
    
    # Check if this is migration or direct creation
    mode = config.get('mode', 'migration')
    
    if mode == 'direct_creation':
        return create_direct_iceberg_table(spark, config)
    else:
        return create_migration_iceberg_table(spark, config)

def create_direct_iceberg_table(spark, config: dict):
    """Create Iceberg table from scratch based on schema definition"""
    
    # Extract configuration
    schema = config['schema']
    partitioning = config['partitioning']
    target_config = config['target']
    iceberg_settings = config['iceberg_settings']
    
    # Set Iceberg catalog properties
    catalog_name = iceberg_settings.get('catalog_name', 'glue_catalog')
    warehouse_location = iceberg_settings.get('warehouse_location')
    
    # Configure Spark for Iceberg
    spark.conf.set(f"spark.sql.catalog.{catalog_name}", "org.apache.iceberg.spark.SparkCatalog")
    spark.conf.set(f"spark.sql.catalog.{catalog_name}.warehouse", warehouse_location)
    spark.conf.set(f"spark.sql.catalog.{catalog_name}.catalog-impl", "org.apache.iceberg.aws.glue.GlueCatalog")
    spark.conf.set(f"spark.sql.catalog.{catalog_name}.io-impl", "org.apache.iceberg.aws.s3.S3FileIO")
    
    # Get target configuration
    target_database = target_config['target_database']
    target_table = target_config['target_table']
    target_s3_location = target_config['target_s3_location']
    
    # Create target database if it doesn't exist
    print(f"Creating target database: {target_database}")
    spark.sql(f"CREATE DATABASE IF NOT EXISTS {catalog_name}.{target_database}")
    
    # Configure Iceberg table properties (optimizations handled by create_table_optimizer API)
    table_properties = {
        'write.format.default': iceberg_settings.get('file_format', 'parquet'),
        'write.target-file-size-bytes': str(iceberg_settings.get('target_file_size', 128) * 1024 * 1024),
        'write.distribution-mode': 'hash',
        'write.parquet.compression-codec': iceberg_settings.get('compression', 'snappy'),
        'history.expire.max-snapshot-age-ms': str(iceberg_settings.get('history_retention_days', 30) * 24 * 60 * 60 * 1000),
        'snapshot.time-retention.millis': str(iceberg_settings.get('snapshot_retention_days', 7) * 24 * 60 * 60 * 1000)
    }
    
    # Add compaction properties if enabled
    if iceberg_settings.get('compaction_enabled', True):
        table_properties.update({
            'write.merge.mode': 'copy-on-write',
            'write.merge.distribution-mode': 'hash',
            'write.target-file-size-bytes': str(iceberg_settings.get('compaction_target_size', 512) * 1024 * 1024),
            'write.merge.mode': 'copy-on-write'
        })
    
    # Create table properties string
    properties_string = ', '.join([f"'{k}' = '{v}'" for k, v in table_properties.items()])
    
    # Create column definitions
    columns = []
    for col in schema:
        col_name = col['name']
        col_type = col['type']
        nullable = "NULL" if col.get('nullable', True) else "NOT NULL"
        comment = f" COMMENT '{col.get('comment', '')}'" if col.get('comment') else ""
        columns.append(f"{col_name} {col_type} {nullable}{comment}")
    
    columns_string = ', '.join(columns)
    
    # Add partitioning if configured
    partition_clause = ""
    if partitioning.get('strategy') != 'none' and partitioning.get('columns'):
        partition_columns = partitioning['columns']
        if partitioning['strategy'] == 'bucket':
            bucket_count = iceberg_settings.get('bucket_count', 10)
            partition_clause = f" PARTITIONED BY BUCKET {bucket_count} ({', '.join(partition_columns)})"
        elif partitioning['strategy'] == 'truncate':
            truncate_width = iceberg_settings.get('truncate_width', 10)
            partition_clause = f" PARTITIONED BY TRUNCATE {truncate_width} ({', '.join(partition_columns)})"
        elif partitioning['strategy'] in ['year', 'month', 'day', 'hour']:
            partition_clause = f" PARTITIONED BY {partitioning['strategy'].upper()} ({', '.join(partition_columns)})"
        else:  # identity
            partition_clause = f" PARTITIONED BY ({', '.join(partition_columns)})"
    
    # Create table SQL
    create_table_sql = f"""
    CREATE TABLE {catalog_name}.{target_database}.{target_table} (
        {columns_string}
    ) USING iceberg
    LOCATION '{target_s3_location}'
    TBLPROPERTIES ({properties_string}){partition_clause}
    """
    
    print("Executing table creation SQL:")
    print(create_table_sql)
    
    spark.sql(create_table_sql)
    
    print(f"Successfully created Iceberg table: {target_database}.{target_table}")
    
    # Check for existing optimizers and enable table optimizations
    print("Checking for existing optimizers...")
    existing_optimizers = check_existing_optimizers(target_database, target_table)
    
    if not existing_optimizers:
        print("Enabling table optimizations...")
        optimization_success = enable_table_optimizations(target_database, target_table, iceberg_settings)
        
        if optimization_success:
            print("Table optimizations enabled successfully")
        else:
            print("Warning: Failed to enable table optimizations")
    else:
        print("Table optimizers already exist, skipping creation")
    
    # Verify optimization status
    verify_optimization_status(target_database, target_table)
    
    return True

def create_migration_iceberg_table(spark, config: dict):
    """Create Iceberg table by migrating from existing Glue table"""
    
    # Extract configuration
    source_config = config['source']
    target_config = config['target']
    iceberg_settings = config['iceberg_settings']
    data_type_mapping = config['data_type_mapping']
    
    # Set Iceberg catalog properties
    catalog_name = iceberg_settings.get('catalog_name', 'glue_catalog')
    warehouse_location = iceberg_settings.get('warehouse_location')
    
    # Configure Spark for Iceberg
    spark.conf.set(f"spark.sql.catalog.{catalog_name}", "org.apache.iceberg.spark.SparkCatalog")
    spark.conf.set(f"spark.sql.catalog.{catalog_name}.warehouse", warehouse_location)
    spark.conf.set(f"spark.sql.catalog.{catalog_name}.catalog-impl", "org.apache.iceberg.aws.glue.GlueCatalog")
    spark.conf.set(f"spark.sql.catalog.{catalog_name}.io-impl", "org.apache.iceberg.aws.s3.S3FileIO")
    
    # Read source table
    source_database = source_config['database']
    source_table = source_config['table']
    
    print(f"Reading source table: {source_database}.{source_table}")
    source_df = spark.table(f"{source_database}.{source_table}")
    
    # Apply data type mappings
    print("Applying data type mappings...")
    for column_name, mapping in data_type_mapping.items():
        glue_type = mapping['glue_type']
        iceberg_type = mapping['iceberg_type']
        
        if glue_type.lower() != iceberg_type.lower():
            print(f"Mapping {column_name}: {glue_type} -> {iceberg_type}")
            # Apply type casting if needed
            if iceberg_type == 'string':
                source_df = source_df.withColumn(column_name, col(column_name).cast(StringType()))
            elif iceberg_type == 'int':
                source_df = source_df.withColumn(column_name, col(column_name).cast(IntegerType()))
            elif iceberg_type == 'long':
                source_df = source_df.withColumn(column_name, col(column_name).cast(LongType()))
            elif iceberg_type == 'float':
                source_df = source_df.withColumn(column_name, col(column_name).cast(FloatType()))
            elif iceberg_type == 'double':
                source_df = source_df.withColumn(column_name, col(column_name).cast(DoubleType()))
            elif iceberg_type == 'boolean':
                source_df = source_df.withColumn(column_name, col(column_name).cast(BooleanType()))
            elif iceberg_type == 'date':
                source_df = source_df.withColumn(column_name, col(column_name).cast(DateType()))
            elif iceberg_type == 'timestamp':
                source_df = source_df.withColumn(column_name, col(column_name).cast(TimestampType()))
    
    # Get target configuration
    target_database = target_config['target_database']
    target_table = target_config['target_table']
    target_s3_location = target_config['target_s3_location']
    
    # Create target database if it doesn't exist
    print(f"Creating target database: {target_database}")
    spark.sql(f"CREATE DATABASE IF NOT EXISTS {catalog_name}.{target_database}")
    
    # Configure Iceberg table properties (optimizations handled by create_table_optimizer API)
    table_properties = {
        'write.format.default': iceberg_settings.get('file_format', 'parquet'),
        'write.target-file-size-bytes': str(iceberg_settings.get('target_file_size', 128) * 1024 * 1024),
        'write.distribution-mode': 'hash',
        'write.parquet.compression-codec': iceberg_settings.get('compression', 'snappy'),
        'history.expire.max-snapshot-age-ms': str(iceberg_settings.get('history_retention_days', 30) * 24 * 60 * 60 * 1000),
        'snapshot.time-retention.millis': str(iceberg_settings.get('snapshot_retention_days', 7) * 24 * 60 * 60 * 1000)
    }
    
    # Enable compaction if configured
    if iceberg_settings.get('compaction_enabled', True):
        table_properties['write.merge.mode'] = 'copy-on-write'
        table_properties['write.merge.distribution-mode'] = 'hash'
    
    # Create table properties string
    properties_string = ', '.join([f"'{k}' = '{v}'" for k, v in table_properties.items()])
    
    # Create Iceberg table
    print(f"Creating Iceberg table: {target_database}.{target_table}")
    
    # Get column definitions
    columns = []
    for column_name, mapping in data_type_mapping.items():
        iceberg_type = mapping['iceberg_type']
        columns.append(f"{column_name} {iceberg_type}")
    
    columns_string = ', '.join(columns)
    
    # Create table SQL
    create_table_sql = f"""
    CREATE TABLE {catalog_name}.{target_database}.{target_table} (
        {columns_string}
    ) USING iceberg
    LOCATION '{target_s3_location}'
    TBLPROPERTIES ({properties_string})
    """
    
    print("Executing table creation SQL:")
    print(create_table_sql)
    
    spark.sql(create_table_sql)
    
    # Write data to Iceberg table
    print("Writing data to Iceberg table...")
    source_df.writeTo(f"{catalog_name}.{target_database}.{target_table}").append()
    
    print(f"Successfully created Iceberg table: {target_database}.{target_table}")
    
    # Verify table creation
    result = spark.sql(f"SELECT COUNT(*) as row_count FROM {catalog_name}.{target_database}.{target_table}")
    row_count = result.collect()[0]['row_count']
    print(f"Table contains {row_count} rows")
    
    # Check for existing optimizers and enable table optimizations
    print("Checking for existing optimizers...")
    existing_optimizers = check_existing_optimizers(target_database, target_table)
    
    if not existing_optimizers:
        print("Enabling table optimizations...")
        optimization_success = enable_table_optimizations(target_database, target_table, iceberg_settings)
        
        if optimization_success:
            print("Table optimizations enabled successfully")
        else:
            print("Warning: Failed to enable table optimizations")
    else:
        print("Table optimizers already exist, skipping creation")
    
    # Verify optimization status
    verify_optimization_status(target_database, target_table)
    
    return True

def main():
    """Main function"""
    try:
        # Get job parameters
        args = getResolvedOptions(sys.argv, ['config_s3_path'])
        config_s3_path = args['config_s3_path']
        
        print(f"Loading configuration from: {config_s3_path}")
        config = load_config_from_s3(config_s3_path)
        
        print("Configuration loaded successfully")
        print(f"Source: {config['source']['database']}.{config['source']['table']}")
        print(f"Target: {config['target']['target_database']}.{config['target']['target_table']}")
        
        # Create Iceberg table
        success = create_iceberg_table(spark, config)
        
        if success:
            print("Migration completed successfully!")
            
            # Monitor optimization jobs
            target_config = config.get('target', {})
            target_database = target_config.get('target_database', '')
            target_table = target_config.get('target_table', '')
            
            if target_database and target_table:
                print("\n" + "="*50)
                print("OPTIMIZATION MONITORING")
                print("="*50)
                monitor_optimization_jobs(target_database, target_table)
                
                # Final optimization status check
                print("\nFinal optimization status:")
                verify_optimization_status(target_database, target_table)
        else:
            print("Migration failed!")
            sys.exit(1)
            
    except Exception as e:
        print(f"Error during migration: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    
    finally:
        job.commit()

if __name__ == "__main__":
    main()

